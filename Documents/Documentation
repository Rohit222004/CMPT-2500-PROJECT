Deep Explanation of the Project Code
This document provides an in-depth explanation of the primary scripts and design decisions in the Days on Market Predictor for Vehicle Sales Optimization project.

1. preprocess.py

Purpose
Handles data cleaning, feature engineering, and preliminary visualizations. Ensures the dataset is in a suitable format for machine learning.

DataPreprocessor Class

Attributes:

 1. file_path (str): Path to the CSV file with raw data.

 2. df (pd.DataFrame): The loaded dataset after being read from CSV.

load_data()

 1. Reads the CSV file using pandas.read_csv.

 2. Prints basic information about the dataset, including shape and a preview.

 3. Returns the loaded DataFrame.

preprocess_data()

 1. Performs data cleaning and feature engineering.

 2. Checks and prints any duplicate rows.

 3. Selects relevant columns for modeling.

 4. Handles anomalous or outlier values in price and msrp by merging group mean values and replacing extreme cases.

 5. Fills missing values for numeric columns using mean values.

 6. Encodes transmission_from_vin as a binary numeric feature.

 7. Visualizes the distribution of transmissions via a pie chart.

 8. Creates a new feature vehicle_age based on current year and model_year.

 9. Converts and extracts components of listing_first_date into year, month, and day.

 10. Returns the cleaned and transformed DataFrame.

save_preprocessed_data(output_file)

 1. Saves the preprocessed DataFrame to a CSV file.

prepare_data()

 1. Splits the final DataFrame into feature set (X) and target (y).

 2. The target is days_on_market.

Notable Design Decisions

 1. The script replaces any suspiciously low price and msrp values (<=1000) with group means, preserving data integrity.

Feature engineering includes:

 1. vehicle_age for each vehicle.

 2. Breakdown of listing date into year, month, and day.

2. train.py

Purpose

Trains a Ridge regression model with hyperparameter tuning on the preprocessed dataset.

ModelTrainer Class

Attributes:

 1. preprocessed_file (str): Path to the cleaned CSV data.

 2. X_train, X_test, y_train, y_test: Train/test splits for features and targets.

 3. best_model: Holds the trained (tuned) Ridge model.

load_data()

 1. Loads the preprocessed data into a DataFrame.

prepare_dataset()

 1. Splits the data into train/test sets with an 80/20 split.

build_pipeline()

 1. Constructs a scikit-learn Pipeline, combining numeric and categorical transformations.

 2. Numeric Transformations: Imputation (SimpleImputer), Scaling (StandardScaler).

 3. Categorical Transformations: One-Hot Encoding (OneHotEncoder).

 4. Regressor: Ridge solver with lsqr.

train()

 1. Loads and splits data.

 2. Builds the pipeline.

 3. Defines param_dist for hyperparameter tuning, using np.logspace for the Ridge alpha.

 4. Utilizes RandomizedSearchCV for random hyperparameter search (20 iterations, 5-fold cross-validation).

 5. Saves the best model (ridge_model.pkl).

 6. Exports X_test and y_test for future evaluation and predictions.

Notable Design Decisions

 1. Ridge Regression selected for its penalty term, which helps handle multicollinearity.

 2. RandomizedSearchCV used for efficiency compared to Grid Search, especially when searching multiple hyperparameters.

 3. The pipeline design ensures data preprocessing (imputation, scaling, encoding) is performed consistently on both training and test data.

3. predict.py

Purpose

Loads the trained model and runs predictions on new (unseen) data.

ModelPredictor Class

Attributes:

 1. model: The Ridge model loaded from disk (joblib.load).

predict(input_data)

 1. Expects a DataFrame with matching column names to the training set.

 2. Returns a NumPy array of predictions.

run_prediction(input_file, output_file)

 1. Loads features from CSV.

 2. Obtains predictions via predict().

 3. Combines input features with predictions.

 4. Saves the resulting DataFrame to a CSV file.

Notable Design Decisions

 1. A dedicated class for predictions allows for stand-alone usage in different environments or integration with APIs.

 2. By expecting a DataFrame, it ensures the column transformations are aligned with how the model was trained.

4. evaluate.py

Purpose

Evaluates model performance on a separate test set, computing metrics such as MSE, RMSE, and R2.

ModelEvaluator Class

Attributes:

 1. model: Loaded Ridge model.

 2. X_test, y_test: Test features and target values loaded from CSV files.

evaluate()

 1. Runs predictions on X_test.

 2. Calculates mean squared error (MSE), root mean squared error (RMSE), and R2 score.

 3. Prints metrics to help gauge model performance.

Notable Design Decisions

 1. The separate script for evaluation decouples training logic from performance measurement.

 2. Allows repeated evaluation of the same model or different model versions.

5. utils/model_utils.py

Purpose

Contains helper functions for model persistence (saving/loading). Useful if you need a consistent approach for model file I/O.

Key Methods

 1. save_model(model, filename)

 2. Uses joblib.dump to save a trained model object to disk.

load_model(filename)

 1. Uses joblib.load to retrieve a saved model.

Notable Design Decisions

 1. Centralizing model loading/saving ensures consistent usage across multiple scripts.

 2. Minimizes duplication of code.



Deep Dive on Design Decisions

Model Choice: Ridge Regression

Reasoning: Ridge is a linear model that includes L2 regularization, helping reduce overfitting by penalizing large coefficient values. The target (days_on_market) is well-suited to a regression approach, and the Ridge penalty helps mitigate potential issues with multicollinearity (high correlation among features).

Feature Engineering

 1. vehicle_age: Directly extracts how old the car is, which is often a predictor for how quickly it sells.

 2. Date Breakdown: year, month, and day from the listing date allow capturing seasonal or monthly effects.

 3. Handling Outliers: Replacing suspiciously low prices and MSRPs with group means ensures more robust training data.

Hyperparameter Tuning

 1. RandomizedSearchCV: Allows more extensive search over alpha values without performing a full grid search. This approach is computationally efficient while maintaining a robust search for the best combination.

Pipeline and ColumnTransformer

 1. Ensures a uniform transformation on both training and unseen data (imputation, scaling, and encoding). This is crucial to avoid data leakage and mismatch between training and inference.

Prediction/Inference Decoupling

 1. predict.py can load the model anywhere—making it easy to integrate into a web service or batch-prediction pipeline.

Conclusion

This project’s modular design (scripts for preprocessing, training, evaluation, and prediction) reflects best practices in MLOps. By separating distinct concerns (cleaning, model building, evaluation), the workflow is maintainable and easily extensible for future iterations or more complex models.

For any questions or issues, please refer to the provided README, or contact the project team.

